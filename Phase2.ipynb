{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data: Project Phase 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By Tormod Alf Try Tufteland and Lars Henrik Berg-Jensen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, Row\n",
    "import os, shutil, datetime\n",
    "from collections import Counter\n",
    "from operator import add, itemgetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master = \"local[4]\"\n",
    "appName = \"phase2\"\n",
    "conf = SparkConf().setAppName(appName).setMaster(master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating RDD from geotweets.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"data/geotweets.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_rdd = rdd.sample(False, 0.1, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating RDD arrays by splitting on tabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_list = rdd.map(lambda x: x.split('\\t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_rdd_list = sampled_rdd.map(lambda x: x.split('\\t'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting place and tweet into new RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using sampled or real RDD list\n",
    "rdd_used = rdd_list\n",
    "#rdd_used = sampled_rdd_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tweet = \"#hiring\"\n",
    "input_tweet_list = input_tweet.split(' ')\n",
    "input_tweet_rdd = sc.parallelize(input_tweet.split(' ')).map(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tweet_count = rdd_used.count()\n",
    "total_tweet_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_and_tweet = rdd_used.map(lambda x: (x[4], x[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_and_tweets = rdd_used.map(lambda x: (x[4], x[10]))\\\n",
    "                    .mapValues(lambda x: x.split(' '))\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findNumberOfTweetsWithWord(dataset, word):\n",
    "    return dataset.filter(lambda x: word in x).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset: rdd of (place, tweets)\n",
    "def findProbabilityOfPlace(dataset, place, tweet_rdd):\n",
    "    tweets_from_place = dataset.filter(lambda x: x[0] == place)\n",
    "    nr_tweets_place = tweets_from_place.count()\n",
    "    tweets_from_place_list = tweets_from_place.map(lambda x: x[1].split(' '))\n",
    "    #word_tweet_count = tweet_rdd.map(lambda x: (x, findNumberOfTweetsWithWord(tweets_from_place_list, x)))\n",
    "    word_tweet_count = list(map(lambda x: (x, findNumberOfTweetsWithWord(tweets_from_place_list, x)), input_tweet_list))\n",
    "    return word_tweet_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_rdd = findProbabilityOfPlace(place_and_tweet, 'Belm, Par', input_tweet_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findPlaceOfTweet(dataset, tweet, output):\n",
    "    number_of_tweets = dataset.count()\n",
    "    \n",
    "    #Split input tweet into list of words and put to lowercase \n",
    "    #For all tweets in dataset for each place, count the number of tweets where word occurs\n",
    "    #For each place in dataset, calculate the probability using naive bayes\n",
    "    #Sort the result by probability\n",
    "    #Return the place with max probability\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_by_city = place_and_tweets.map(lambda x: (x[0], list(map(str.lower, x[1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_per_place = place_and_tweet.map(lambda x: (x[0], 1)).countByKey().items()\n",
    "tweets_per_place = sc.parallelize(tweets_per_place).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addIfWordInTweet(word, value, tweet_list):\n",
    "    if word in tweet_list:\n",
    "        value += 1\n",
    "    return value\n",
    "        \n",
    "def to_dict(tweet_list):\n",
    "    counter_dict = {k: 0 for k in input_tweet_list}\n",
    "    #counter_dict = {k: func(tweet_list, k) for k in new_dict.items()}\n",
    "    return counter_dict\n",
    "    \n",
    "def add(counter_dict, tweet_list):\n",
    "    return {k: addIfWordInTweet(k, v, tweet_list) for k, v in counter_dict.items()}\n",
    "\n",
    "\n",
    "def merge(dict1, dict2):\n",
    "    new = {**dict1, **dict1}\n",
    "    return new\n",
    "\n",
    "counted_tweets_with_word_by_city = tweets_by_city.combineByKey(to_dict, add, merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counted_tweets_with_word_by_city.collect()[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset: rdd of (place, {word1: count, word2:count ...})\n",
    "def findProbabilityOfPlace(dataset, place):\n",
    "    nr_tweets_place = tweets_per_place[place]\n",
    "    probability = dataset.filter(lambda x: x == place)\n",
    "    probability = (nr_tweets_place / total_tweet_count) * \n",
    "    return word_tweet_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "def calculateProbability(place, data_dict):\n",
    "    nr_tweets_place = tweets_per_place[place]\n",
    "    initial_value = nr_tweets_place / total_tweet_count \n",
    "    probability = reduce(lambda x, value: x * value / nr_tweets_place, data_dict.values(), initial_value)\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findProbabilities(dataset):\n",
    "    probabilities = dataset.map(lambda x: (x[0], calculateProbability(x[0], x[1])))\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = findProbabilities(counted_tweets_with_word_by_city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_probabilities = sorted(probabilities.collect(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities.max(key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (BigData_spring2018)",
   "language": "python",
   "name": "bigdata_spring2018"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
