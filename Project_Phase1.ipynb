{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, Row\n",
    "import os, shutil, datetime\n",
    "from collections import Counter\n",
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/env python3\"\n",
    "master = \"local[4]\"\n",
    "appName = \"phase1\"\n",
    "conf = SparkConf().setAppName(appName).setMaster(master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"data/geotweets.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a sample RDD for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_rdd = rdd.sample(False, 0.1, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating array splitting on tabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_list = rdd.map(lambda x: x.split('\\t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rdd_list.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_rdd_list = sampled_rdd.map(lambda x: x.split('\\t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list = sampled_rdd_list.keyBy(lambda x: x[1])\n",
    "print(new_list.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_tweets = rdd.count()\n",
    "print(number_of_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_users = rdd_list.map(lambda x: x[6]).distinct().count()\n",
    "print(number_of_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_countries = rdd_list.map(lambda x: x[1]).distinct().count()\n",
    "print(number_of_countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_places = rdd_list.map(lambda x: x[4]).distinct().count()\n",
    "print(number_of_places)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number_of_languages = rdd_list.map(lambda x: (x, 1) ).reduce(lambda a, b: a+b)\n",
    "number_of_languages = rdd_list.map(lambda x: x[5]).distinct().count()\n",
    "print(number_of_languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_latitude = rdd_list.map(lambda x: float(x[11])).reduce(lambda a, b: min(a,b))\n",
    "print(minimum_latitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_longitude = rdd_list.map(lambda x: float(x[12])).reduce(lambda a, b: min(a,b))\n",
    "print(minimum_longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_latitude = rdd_list.map(lambda x: float(x[11])).reduce(lambda a, b: max(a,b))\n",
    "print(maximum_latitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_longitude = rdd_list.map(lambda x: float(x[12])).reduce(lambda a, b: max(a,b))\n",
    "#alsjdk\n",
    "print(maximum_longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text = rdd_list.map(lambda x: x[10])\n",
    "print(tweet_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_in_characters = tweet_text.map(lambda x: (len(x)))\n",
    "average_tweet_in_characters = tweet_in_characters.mean()\n",
    "print(average_tweet_in_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_in_words = tweet_text.map(lambda x: len(x.split(' ')))\n",
    "average_tweet_in_words = tweet_in_words.mean()\n",
    "print(average_tweet_in_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining results to RDD and writes to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = sc.parallelize([number_of_tweets, number_of_users,\\\n",
    "                        number_of_countries, number_of_places,\\\n",
    "                        number_of_languages, minimum_latitude,\\\n",
    "                        minimum_longitude, maximum_latitude, maximum_longitude,\\\n",
    "                        average_tweet_in_characters, average_tweet_in_words])\n",
    "results = results.coalesce(1)\n",
    "resultsPath = 'results/result_1.tsv'\n",
    "if os.path.isdir(resultsPath):\n",
    "    shutil.rmtree(resultsPath)\n",
    "results_tsv = results.saveAsTextFile(resultsPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creates new RDD by MapReduce, counting number of tweets per country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_rdd = rdd_list.map(lambda x: (str(x[1]), 1)).countByKey().items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorts twice. First alphabetically ascending on country name, then numerically descending on number of tweets. We can do this since the sorts are stable, hence the order between records with same key is preserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dict = sorted(new_rdd, key=lambda x: x[0])\n",
    "sorted_dict = sorted(sorted_dict, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving result as RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_task2_rdd = sc.parallelize(sorted_dict)\n",
    "result_task2 = result_task2_rdd.map(lambda x: '{}\\t{}'.format(x[0],x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing results to text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsPath = 'results/result_2.tsv'\n",
    "if os.path.isdir(resultsPath):\n",
    "    shutil.rmtree(resultsPath)\n",
    "result_task2.coalesce(1).saveAsTextFile(resultsPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_under_10 = result_task2_rdd.filter(lambda x: x[1] < 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_with_lat = rdd_list.map(lambda x: (str(x[1]), float(x[11])))\n",
    "countries_with_lon = rdd_list.map(lambda x: (str(x[1]), float(x[12])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_over_10_with_lat = countries_with_lat.subtractByKey(countries_under_10)\n",
    "countries_over_10_with_lon = countries_with_lon.subtractByKey(countries_under_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateCenter(listWithCoord):\n",
    "    return sum(listWithCoord)/len(listWithCoord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_centroid_lat = countries_over_10_with_lat.groupByKey().\\\n",
    "                    mapValues(list).mapValues(calculateCenter)\n",
    "country_centroid_lon = countries_over_10_with_lon.groupByKey().\\\n",
    "                    mapValues(list).mapValues(calculateCenter)\n",
    "country_centroid_rdd = country_centroid_lat.join(country_centroid_lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_task3 = country_centroid_rdd.map(lambda x: '{}\\t{}\\t{}'.format(x[0], x[1][0], x[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsPath = 'results/result_3.tsv'\n",
    "if os.path.isdir(resultsPath):\n",
    "    shutil.rmtree(resultsPath)\n",
    "result_task3.coalesce(1).saveAsTextFile(resultsPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cartoframes import Layer, BaseMap, styling\n",
    "BASEURL = 'https://larshbj.carto.com'\n",
    "APIKEY = '299d2d825191b9879da6fc859d1064930f28d061'\n",
    "cc = cartoframes.CartoContext(base_url=BASEURL,\n",
    "                              api_key=APIKEY)\n",
    "cc.map(layers=Layer('result_task3_carto_4',\n",
    "                   size=7),\n",
    "       interactive=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLocalTimeHour(timestamp, offset):\n",
    "    s = timestamp / 1000.0 + offset\n",
    "    return str(datetime.datetime.fromtimestamp(s).hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMaxTweetTimeInterval(hour_list):\n",
    "    result = Counter(hour_list).most_common(1)\n",
    "    return result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_task4 = rdd_list.map(lambda x: (str(x[1]), getLocalTimeHour(float(x[0]), float(x[8]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can be made more efficient using reduceByKey\n",
    "country_time_rdd = rdd_task4.groupByKey().mapValues(lambda x: list(x))\\\n",
    "                    .mapValues(lambda x: getMaxTweetTimeInterval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_task4 = country_time_rdd.map(lambda x: '{}\\t{}\\t{}'.format(x[0], x[1][0], x[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsPath = 'results/result_4.tsv'\n",
    "if os.path.isdir(resultsPath):\n",
    "    shutil.rmtree(resultsPath)\n",
    "result_task4.coalesce(1).saveAsTextFile(resultsPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findNumberOfTweetsAndSort(rdd):\n",
    "    result = rdd.map(lambda x: (str(x[4]), 1)).countByKey().items()\n",
    "    result = sorted(result, key=lambda x: x[0])\n",
    "    return sorted(result, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_task5 = rdd_list.filter(lambda x: x[2] == 'US' and x[3] == 'city')\n",
    "rdd_task5 = findNumberOfTweetsAndSort(rdd_task5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_task5_rdd = sc.parallelize(rdd_task5)\n",
    "result_task5 = result_task5_rdd.map(lambda x: '{}\\t{}'.format(x[0],x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsPath = 'results/result_5.tsv'\n",
    "if os.path.isdir(resultsPath):\n",
    "    shutil.rmtree(resultsPath)\n",
    "result_task5.coalesce(1).saveAsTextFile(resultsPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_rdd = sc.textFile(\"data/stop_words.txt\")\n",
    "stopwords_list = stopwords_rdd.flatMap(lambda x: str(x).split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterTweetsByLength(rdd):\n",
    "    return rdd.filter(lambda x: len(x) >= 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterTweetsByStopwords(rdd, stopwords):\n",
    "    return rdd.subtract(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_task6_tweets = rdd_list.filter(lambda x: x[2] == 'US')\\\n",
    "                    .map(lambda x: str(x[10]))\\\n",
    "                    .flatMap(lambda x: x.split(' '))\n",
    "print(rdd_task6_tweets.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task6_freq_words_list = rdd_task6_tweets.filter(lambda x: len(x) >= 2)\\\n",
    "                    .map(lambda x: x.lower())\\\n",
    "                    .subtract(stopwords_list)\\\n",
    "                    .map(lambda x: (x, 1))\\\n",
    "                    .reduceByKey(add)\\\n",
    "                    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task6_freq_words_list_sorted = sorted(task6_freq_words_list, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_task6 = sc.parallelize(task6_freq_words_list_sorted[0:10])\\\n",
    "                    .map(lambda x: '{}\\t{}'.format(x[0], x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsPath = 'results/result_6.tsv'\n",
    "if os.path.isdir(resultsPath):\n",
    "    shutil.rmtree(resultsPath)\n",
    "result_task6.coalesce(1).saveAsTextFile(resultsPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_cities = result_task5_rdd.zipWithIndex()\\\n",
    "                .filter(lambda index: index[1] < 5).keys()\n",
    "#print(five_cities.collect())\n",
    "tweet_text = rdd_list.map(lambda x: (x[4], x[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = stopwords_list.map(lambda x: (0, x))\n",
    "\n",
    "tweets_by_city = tweet_text.join(five_cities)\\\n",
    "                        .map(lambda x: (x[0], x[1][0]))\\\n",
    "                        .flatMapValues(lambda x: x.split(' '))\\\n",
    "                        .filter(lambda x: len(x[1]) >= 2)\\\n",
    "                        .map(lambda x: (x[0], x[1].lower()))\\\n",
    "                        .subtract(sub)\n",
    "#print(tweets_by_city.take(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dict(word):\n",
    "    city = {}\n",
    "    city[word] = 1\n",
    "    return city\n",
    "    \n",
    "def add(city, word):\n",
    "    if word in city:\n",
    "        city[word] += 1\n",
    "    else:\n",
    "        city[word] = 1\n",
    "    return city\n",
    "\n",
    "def merge(dict1, dict2):\n",
    "    new = {**dict1, **dict1}\n",
    "    return new\n",
    "\n",
    "counted_tweets_by_city = tweets_by_city.combineByKey(to_dict, add, merge)\\\n",
    "        .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "common_words = []\n",
    "for city in counted_tweets_by_city:\n",
    "    sorted_words = sorted(city[1].items(), key=operator.itemgetter(1), reverse=True)[0:10]\n",
    "    c = []\n",
    "    for word_tuple in sorted_words:\n",
    "        c.append('\\t'.join(map(str,word_tuple)))\n",
    "    d = '\\t'.join(c)\n",
    "    common_words.append((city[0], d))\n",
    "#print(common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_task7 = sc.parallelize(common_words)\\\n",
    "                    .map(lambda x: '{}\\t{}'.format(x[0], x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsPath = 'results/result_7.tsv'\n",
    "if os.path.isdir(resultsPath):\n",
    "    shutil.rmtree(resultsPath)\n",
    "result_task7.coalesce(1).saveAsTextFile(resultsPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"phase1_dataframe\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts = rdd.map(lambda l: l.split('\\t'))\n",
    "tweets = parts.map(lambda x: Row(\\\n",
    "                                utc_time=x[0],\\\n",
    "                                country_name=x[1],\\\n",
    "                                country_code=x[2],\\\n",
    "                                place_type=x[3],\\\n",
    "                                place_name=x[4],\\\n",
    "                                language=x[5],\\\n",
    "                                username=x[6],\\\n",
    "                                user_screen_name=x[7],\\\n",
    "                                timezone_offset=x[8],\\\n",
    "                                number_of_friends=x[9],\\\n",
    "                                tweet_text=x[10],\\\n",
    "                                latitude=x[11],\\\n",
    "                                longitude=x[12]\\\n",
    "                                ))\n",
    "df = spark.createDataFrame(tweets)\n",
    "df.createOrReplaceTempView(\"tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "    select count(*) as number_of_tweets,\n",
    "        count(distinct(username)) as distinct_users,\n",
    "        count(distinct(country_name)) as distinct_countries,\n",
    "        count(distinct(place_name)) as distinct_places,\n",
    "        count(distinct(country_name)) as distinct_languages,\n",
    "        min(latitude) as minimum_latitude,\n",
    "        min(longitude) as minimum_longitude,\n",
    "        max(latitude) as maximum_latitude,\n",
    "        max(longitude) as maximum_longitude    \n",
    "    from tweets\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "df_sql = spark.sql(sql)\n",
    "df_sql.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
