{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Big Data: Project phase 1\n",
        "# Data Analysis with Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession, Row\n",
        "import os, shutil, datetime\n",
        "from collections import Counter\n",
        "from operator import add, itemgetter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Creating Spark Context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "master = \"local[4]\"\n",
        "appName = \"phase1\"\n",
        "conf = SparkConf().setAppName(appName).setMaster(master)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sc = SparkContext(conf=conf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sc.setLogLevel(\"WARN\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Creating RDD from geotweets.tsv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rdd = sc.textFile(\"data/geotweets.tsv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Creating a sample RDD for testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sampled_rdd = rdd.sample(False, 0.1, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Creating RDD arrays by splitting on tabs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rdd_list = rdd.map(lambda x: x.split('\\t'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sampled_rdd_list = sampled_rdd.map(lambda x: x.split('\\t'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RDD API Tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Counting number of tweets by number of elements in RDD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "number_of_tweets = rdd.count()\n",
        "print number_of_tweets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Counting number of users by extracting username column using \"map\" and counting distinct names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "number_of_users = rdd_list.map(lambda x: x[6]).distinct().count()\n",
        "print number_of_users"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Same procedure for countries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "number_of_countries = rdd_list.map(lambda x: x[1]).distinct().count()\n",
        "print number_of_countries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Same procedure for places"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "number_of_places = rdd_list.map(lambda x: x[4]).distinct().count()\n",
        "print number_of_places"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Same procedure for languages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "number_of_languages = rdd_list.map(lambda x: x[5]).distinct().count()\n",
        "print number_of_languages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Computing min/max latitude and longitude by extracting appropriate columns by mapping and then reducing using min/max methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "minimum_latitude = rdd_list.map(lambda x: float(x[11])).reduce(lambda a, b: min(a,b))\n",
        "print minimum_latitude"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "minimum_longitude = rdd_list.map(lambda x: float(x[12])).reduce(lambda a, b: min(a,b))\n",
        "print minimum_longitude"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "maximum_latitude = rdd_list.map(lambda x: float(x[11])).reduce(lambda a, b: max(a,b))\n",
        "print maximum_latitude"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "maximum_longitude = rdd_list.map(lambda x: float(x[12])).reduce(lambda a, b: max(a,b))\n",
        "print maximum_longitude"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Extracting tweet texts "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tweet_text = rdd_list.map(lambda x: x[10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Computing number of characters in tweets by mapping to length of tweet, and then calculating the mean value of RDD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tweet_in_characters = tweet_text.map(lambda x: (len(x)))\n",
        "average_tweet_in_characters = tweet_in_characters.mean()\n",
        "print average_tweet_in_characters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Same procedure, but first splitting the tweets on space character to get length of words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tweet_in_words = tweet_text.map(lambda x: len(x.split(' ')))\n",
        "average_tweet_in_words = tweet_in_words.mean()\n",
        "print average_tweet_in_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Combining results by parallelizing to RDD and writing to file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = sc.parallelize([number_of_tweets, number_of_users,\\\n",
        "                        number_of_countries, number_of_places,\\\n",
        "                        number_of_languages, minimum_latitude,\\\n",
        "                        minimum_longitude, maximum_latitude, maximum_longitude,\\\n",
        "                        average_tweet_in_characters, average_tweet_in_words])\n",
        "results = results.coalesce(1)\n",
        "resultsPath = 'results/result_1.tsv'\n",
        "if os.path.isdir(resultsPath):\n",
        "    shutil.rmtree(resultsPath)\n",
        "results_tsv = results.saveAsTextFile(resultsPath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Creates new RDD by MapReduce, counting number of tweets per country"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tweets_per_country = rdd_list.map(lambda x: (str(x[1]), 1)).countByKey().items()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Sorts rdd twice. First alphabetically ascending on country name, then numerically descending on number of tweets. We can do this since the sorts are stable, hence the order between records with same key is preserved"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tweets_per_country_sorted = sorted(tweets_per_country, key=lambda x: x[0])\n",
        "tweets_per_country_sorted = sorted(tweets_per_country_sorted, key=lambda x: x[1], reverse=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Saving results as RDD and formatting to correct format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result_task2_rdd = sc.parallelize(sorted_dict)\n",
        "result_task2 = result_task2_rdd.map(lambda x: '{}\\t{}'.format(x[0],x[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Writing results to text file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "resultsPath = 'results/result_2.tsv'\n",
        "if os.path.isdir(resultsPath):\n",
        "    shutil.rmtree(resultsPath)\n",
        "result_task2.coalesce(1).saveAsTextFile(resultsPath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Getting countries with 10 tweets or less by filtering results from task 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "countries_under_10 = result_task2_rdd.filter(lambda x: x[1] < 11)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Mapping original rdd to two new RDDs, keyed by country name and latitude and longtitude values, respectively"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "countries_with_lat = rdd_list.map(lambda x: (str(x[1]), float(x[11])))\n",
        "countries_with_lon = rdd_list.map(lambda x: (str(x[1]), float(x[12])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Getting RDDs with latitude and longitude of countries with over 10 tweets by subtracting by key with countries_under_10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "countries_over_10_with_lat = countries_with_lat.subtractByKey(countries_under_10)\n",
        "countries_over_10_with_lon = countries_with_lon.subtractByKey(countries_under_10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Method for calculating center coordinate. Input is a list of latitudes or longitudes. Output is center point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculateCenter(listWithCoord):\n",
        "    return sum(listWithCoord)/len(listWithCoord)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Computing centroid latitude and longtitude for each country by grouping coords by key, converting to list and then perform calculateCenter on each list. Then joining the two RDDs to a new RDD with both latitude and longitude centroid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "country_centroid_lat = countries_over_10_with_lat.groupByKey().\\\n",
        "                    mapValues(list).mapValues(calculateCenter)\n",
        "country_centroid_lon = countries_over_10_with_lon.groupByKey().\\\n",
        "                    mapValues(list).mapValues(calculateCenter)\n",
        "country_centroid_rdd = country_centroid_lat.join(country_centroid_lon)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Formatting results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result_task3 = country_centroid_rdd.map(lambda x: '{}\\t{}\\t{}'.format(x[0], x[1][0], x[1][1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Savings results to file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "resultsPath = 'results/result_3.tsv'\n",
        "if os.path.isdir(resultsPath):\n",
        "    shutil.rmtree(resultsPath)\n",
        "result_task3.coalesce(1).saveAsTextFile(resultsPath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Using cartoframes library to visualize cartoDB map in notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cartoframes\n",
        "from cartoframes import Layer, BaseMap, styling\n",
        "BASEURL = 'https://larshbj.carto.com'\n",
        "APIKEY = '299d2d825191b9879da6fc859d1064930f28d061'\n",
        "cc = cartoframes.CartoContext(base_url=BASEURL,\n",
        "                              api_key=APIKEY)\n",
        "cc.map(layers=Layer('result_task3_carto_4',\n",
        "                   size=7),\n",
        "       interactive=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Method for calculating local time by converting timestamp to UTC and adding timezone offset. Outputs time rounded to the hour"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getLocalTimeHour(timestamp, offset):\n",
        "    s = timestamp / 1000.0 + offset\n",
        "    return str(datetime.datetime.fromtimestamp(s).hour)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Method using Python Counter class to calculate 1-hour interval with most tweets. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getMaxTweetTimeInterval(hour_list):\n",
        "    result = Counter(hour_list).most_common(1)\n",
        "    return result[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Extracting country name and calculating local time of tweet using getLocalTimeHour"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rdd_task4 = rdd_list.map(lambda x: (str(x[1]), getLocalTimeHour(float(x[0]), float(x[8]))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Calculating max 1-hour interval per country by grouping tweet hours in lists by key and performing getMaxTweetTimeInterval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This can be made more efficient using reduceByKey\n",
        "country_time_rdd = rdd_task4.groupByKey().mapValues(lambda x: list(x))\\\n",
        "                    .mapValues(lambda x: getMaxTweetTimeInterval(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Formatting results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result_task4 = country_time_rdd.map(lambda x: '{}\\t{}\\t{}'.format(x[0], x[1][0], x[1][1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Saving results to file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "resultsPath = 'results/result_4.tsv'\n",
        "if os.path.isdir(resultsPath):\n",
        "    shutil.rmtree(resultsPath)\n",
        "result_task4.coalesce(1).saveAsTextFile(resultsPath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Method for finding number of tweets of RDD and sorts in descending and alphabetical order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def findNumberOfTweetsAndSort(rdd):\n",
        "    result = rdd.map(lambda x: (str(x[4]), 1)).countByKey().items()\n",
        "    result = sorted(result, key=lambda x: x[0])\n",
        "    return sorted(result, key=lambda x: x[1], reverse=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Filterting RDD on country code 'US' and place type 'city' and computing number of tweets and sorts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rdd_task5 = rdd_list.filter(lambda x: x[2] == 'US' and x[3] == 'city')\n",
        "rdd_task5 = findNumberOfTweetsAndSort(rdd_task5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Formatting results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result_task5_rdd = sc.parallelize(rdd_task5)\n",
        "result_task5 = result_task5_rdd.map(lambda x: '{}\\t{}'.format(x[0],x[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Savings results to file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "resultsPath = 'results/result_5.tsv'\n",
        "if os.path.isdir(resultsPath):\n",
        "    shutil.rmtree(resultsPath)\n",
        "result_task5.coalesce(1).saveAsTextFile(resultsPath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Creating RDD from stop words file and converting to list by splitting on line break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stopwords_rdd = sc.textFile(\"data/stop_words.txt\")\n",
        "stopwords_list = stopwords_rdd.flatMap(lambda x: str(x).split('\\n'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Creating RDD with a list of words from US tweets "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rdd_task6_tweets = rdd_list.filter(lambda x: x[2] == 'US')\\\n",
        "                    .map(lambda x: str(x[10]))\\\n",
        "                    .flatMap(lambda x: x.split(' '))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Filter this RDD on word length, make all words lower case, subtract the stop words and finally MapReduce to count frequency of all words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "task6_freq_words_list = rdd_task6_tweets.filter(lambda x: len(x) >= 2)\\\n",
        "                    .map(lambda x: x.lower())\\\n",
        "                    .subtract(stopwords_list)\\\n",
        "                    .map(lambda x: (x, 1))\\\n",
        "                    .reduceByKey(add)\\\n",
        "                    .collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Sorts frequency of words in descending order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "task6_freq_words_list_sorted = sorted(task6_freq_words_list, key=lambda x: x[1], reverse=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Save the 10 most frequent words as RDD and format results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result_task6 = sc.parallelize(task6_freq_words_list_sorted[0:10])\\\n",
        "                    .map(lambda x: '{}\\t{}'.format(x[0], x[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Save results to file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "resultsPath = 'results/result_6.tsv'\n",
        "if os.path.isdir(resultsPath):\n",
        "    shutil.rmtree(resultsPath)\n",
        "result_task6.coalesce(1).saveAsTextFile(resultsPath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Find the 5 cities with highest number of tweets by filtering results from task 5 and keeping the keys/city names. Create new RDD with city name as key and tweet text as value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "five_cities = result_task5_rdd.zipWithIndex()\\\n",
        "                .filter(lambda index: index[1] < 5).keys()\n",
        "tweet_text = rdd_list.map(lambda x: (x[4], x[10]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Join the two previous RDDs to get a RDD of the five cities with tweet. Split tweet texts into word lists and filter and subtract as in task 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sub = stopwords_list.map(lambda x: (0, x))\n",
        "tweets_by_city = tweet_text.join(five_cities)\\\n",
        "                        .map(lambda x: (x[0], x[1][0]))\\\n",
        "                        .flatMapValues(lambda x: x.split(' '))\\\n",
        "                        .filter(lambda x: len(x[1]) >= 2)\\\n",
        "                        .map(lambda x: (x[0], x[1].lower()))\\\n",
        "                        .subtract(sub)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Methods used in CombineByKey"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def to_dict(word):\n",
        "    city = {}\n",
        "    city[word] = 1\n",
        "    return city\n",
        "    \n",
        "def add(city, word):\n",
        "    if word in city:\n",
        "        city[word] += 1\n",
        "    else:\n",
        "        city[word] = 1\n",
        "    return city\n",
        "\n",
        "def merge(dict1, dict2):\n",
        "    new = {**dict1, **dict1}\n",
        "    return new\n",
        "\n",
        "counted_tweets_by_city = tweets_by_city.combineByKey(to_dict, add, merge)\\\n",
        "        .collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "common_words = []\n",
        "for city in counted_tweets_by_city:\n",
        "    sorted_words = sorted(city[1].items(), key=itemgetter(1), reverse=True)[0:10]\n",
        "    c = []\n",
        "    for word_tuple in sorted_words:\n",
        "        c.append('\\t'.join(map(str,word_tuple)))\n",
        "    d = '\\t'.join(c)\n",
        "    common_words.append((city[0], d))\n",
        "#print(common_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result_task7 = sc.parallelize(common_words)\\\n",
        "                    .map(lambda x: '{}\\t{}'.format(x[0], x[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "resultsPath = 'results/result_7.tsv'\n",
        "if os.path.isdir(resultsPath):\n",
        "    shutil.rmtree(resultsPath)\n",
        "result_task7.coalesce(1).saveAsTextFile(resultsPath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Creating SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .master(\"local\") \\\n",
        "    .appName(\"phase1_dataframe\") \\\n",
        "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Creating Dataframe using the original RDD of geotweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "parts = rdd.map(lambda l: l.split('\\t'))\n",
        "tweets = parts.map(lambda x: Row(\\\n",
        "                                utc_time=x[0],\\\n",
        "                                country_name=x[1],\\\n",
        "                                country_code=x[2],\\\n",
        "                                place_type=x[3],\\\n",
        "                                place_name=x[4],\\\n",
        "                                language=x[5],\\\n",
        "                                username=x[6],\\\n",
        "                                user_screen_name=x[7],\\\n",
        "                                timezone_offset=x[8],\\\n",
        "                                number_of_friends=x[9],\\\n",
        "                                tweet_text=x[10],\\\n",
        "                                latitude=x[11],\\\n",
        "                                longitude=x[12]\\\n",
        "                                ))\n",
        "df = spark.createDataFrame(tweets)\n",
        "df.createOrReplaceTempView(\"tweets\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sql = \"\"\"\n",
        "    select count(*) as number_of_tweets,\n",
        "        count(distinct(username)) as distinct_users,\n",
        "        count(distinct(country_name)) as distinct_countries,\n",
        "        count(distinct(place_name)) as distinct_places,\n",
        "        count(distinct(country_name)) as distinct_languages,\n",
        "        min(latitude) as minimum_latitude,\n",
        "        min(longitude) as minimum_longitude,\n",
        "        max(latitude) as maximum_latitude,\n",
        "        max(longitude) as maximum_longitude    \n",
        "    from tweets\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "df_sql = spark.sql(sql)\n",
        "df_sql.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
